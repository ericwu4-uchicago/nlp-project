{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3b0f46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.naive_bayes import GaussianNB, ComplementNB\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1ff0f591",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = np.array([1,2,2,2,2,1,1,1,1,3,1,1,1,3,1,1,1,3,3,3,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,\n",
    "        1,1,1,3,3,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1])\n",
    "known = [True] * 85\n",
    "known[63] = False\n",
    "known[17:20] = [False] * 3\n",
    "known[48:58] = [False] * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "18ed48b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    texts = []\n",
    "    textprepath = \"Federalist Papers/fed paper \"\n",
    "    texts = []\n",
    "    for i in range(85):\n",
    "        f = open(f\"{textprepath}{i + 1}.txt\", \"r\")\n",
    "        text = f.read()\n",
    "        parts = text.split(\"\\n\", 1) #split out the first line\n",
    "        f.close()\n",
    "        texts.append({'text':parts[1]})\n",
    "    df = pd.DataFrame(texts)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "374ec42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data()\n",
    "train = known and [i % 8 != 0 for i in range(len(known))]\n",
    "test = known and [i % 8 == 0 for i in range(len(known))]\n",
    "train_df = data[train]\n",
    "test_df = data[test]\n",
    "codestrain = codes[train]\n",
    "codestest = codes[test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4ba44862",
   "metadata": {},
   "outputs": [],
   "source": [
    "#authors are tolstoy, beliaev, teffi, danilevskii, averchenko, chiornyi\n",
    "#authors = np.repeat([1, 2, 2, 3, 4, 5, 4, 5, 6], 3)\n",
    "authors = np.repeat([1, 3, 2, 2, 3, 3, 2, 2, 3, 5, 4, 5, 3, 5, 2, 5, 6, 3, 5], 3)\n",
    "#huggingface, google, deepl\n",
    "translators = np.repeat([1, 2, 3],19)\n",
    "fullds =[] \n",
    "directory = \"russian short stories\"\n",
    "directories = ['init translations', 'google translation', 'deepl translations']\n",
    "#directory = \"test translation\"\n",
    "#files = [\"Сказка о лысом пророке Елисее chiornyi.txt\"]\n",
    "for directory in directories:\n",
    "    #print(directory)\n",
    "    for file in os.listdir(directory):\n",
    "#for file in files:\n",
    "        filename = os.fsdecode(file)\n",
    "        if filename.endswith(\".txt\"):\n",
    "            #print(filename)\n",
    "            #f = open(f\"{directory}/{filename}\", encoding='utf-8') #'r',\n",
    "            f = open(f\"{directory}/{filename}\", 'r',encoding='utf-8')\n",
    "            text = f.read()\n",
    "            f.close()\n",
    "            fullds.append({'text':text})\n",
    "            #fullds.append(text)\n",
    "translated_df = pd.DataFrame(fullds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b579785e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#intrain = [i % 5 != 0 for i in range(translated_df.size)]\n",
    "intrain = [i % 9 > 2 for i in range(translated_df.size)]\n",
    "#intrain = [True, True, True, True, True, True, False, False, False,\n",
    "#          True, True, True, False, False, False, True, True, True, \n",
    "#          False, False, False, True, True, True, True, True, True,]\n",
    "#notintrain = [i % 9 <= 2 for i in range(translated_df.size)]\n",
    "notintrain = [not element for element in intrain]\n",
    "train_df = translated_df[intrain]\n",
    "test_df = translated_df[notintrain]\n",
    "codestrain = authors[intrain]\n",
    "#codestrain = translators[intrain]\n",
    "codestest = authors[notintrain]\n",
    "#codestest = translators[notintrain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f0e72054",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_iter = iter(train_df['text'])\n",
    "test_iter = iter(test_df['text'])\n",
    "def yield_tokens(train_iter):\n",
    "    for text in train_iter:\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "67d6d18e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nConcernin'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_iter)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "202b6497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only inlcude words that occur at least 10 times in the training data.\n",
    "# Also let \"<unk>\" represent unknown words, i.e., words not in the vocabulary.\n",
    "#vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"], min_freq=10)\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"], min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0058ddd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "# It is best to save GloVe data in a cache to reuse across projects.\n",
    "VECTOR_CACHE_DIR = '.vector_cache'\n",
    "\n",
    "glove = GloVe(name='6B', cache = VECTOR_CACHE_DIR)\n",
    "words = [\"hello\", \"hi\", \"king\", \"president\"]\n",
    "vecs = glove.get_vecs_by_tokens(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0c3d2fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getfscore(clf, labels, inputs):\n",
    "    pred = clf.predict(inputs)\n",
    "    f = f1_score(labels, pred, average='micro')\n",
    "    print(pred)\n",
    "    print(labels)\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a99c316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_into_cnbow(batch):\n",
    "    k = len(batch)\n",
    "    m = vecs.shape[1]\n",
    "    tt = torch.zeros([k,m])#, dtype = torch.float)\n",
    "    #for each word get pretrained embeddings\n",
    "    for i in range(k):\n",
    "        tt[i] = sum(glove.get_vecs_by_tokens(batch[i].split()))\n",
    "    #print(tt[0,:5]/max(torch.abs(tt[1])))\n",
    "    return torch.nn.functional.normalize(tt, dim = 1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef67fb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_into_cbow(batch):\n",
    "    k = len(batch)\n",
    "    m = vecs.shape[1]\n",
    "    tt = torch.zeros([k,m])#, dtype = torch.float)\n",
    "    #for each word get pretrained embeddings\n",
    "    for i in range(k):\n",
    "        tt[i] = sum(glove.get_vecs_by_tokens(batch[i].split()))\n",
    "    return tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5faaebff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_into_bow(batch):\n",
    "    k = len(batch)\n",
    "    m = vocab.__len__()\n",
    "    tt = torch.zeros([k,m])#, dtype = torch.float)\n",
    "    for i in range(k):\n",
    "        words = batch[i].split()\n",
    "        for word in words:\n",
    "            if (vocab.__contains__(word)):\n",
    "                tt[i][vocab.__getitem__(word)] += 1/len(words)\n",
    "    return tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "69d6241a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "tensor([[ -97.5652,   86.8800,  -21.2476,  ..., -101.2449, -103.1667,\n",
      "           46.4730],\n",
      "        [-129.7837,  104.1886,  -45.2938,  ..., -174.8952,  -90.4316,\n",
      "          104.2629],\n",
      "        [-233.7854,  208.7918, -126.9739,  ..., -269.0349, -219.9509,\n",
      "          117.1081],\n",
      "        ...,\n",
      "        [-126.4933,  107.4950,  -49.5160,  ..., -153.6434, -103.4746,\n",
      "           73.0541],\n",
      "        [-126.1103,  102.3025,  -53.6391,  ..., -200.8259, -105.9677,\n",
      "          102.4004],\n",
      "        [ -88.5561,  103.8365,  -20.1227,  ..., -151.1780,  -84.3824,\n",
      "           59.2007]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(test_df['text']))\n",
    "print(collate_into_cbow(list(test_df['text'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a0366c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 3 3 1 1 1]\n",
      "[1 1 1 1 1 3 3 3 1 1 1]\n",
      "0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "#translators/federalist papers\n",
    "#xtrain = collate_into_cbow(list(train_df['text']))\n",
    "xtrain = collate_into_cbow(list(train_df['text']))\n",
    "clf = LogisticRegression(random_state=0, max_iter = 1000).fit(xtrain, codestrain)\n",
    "xtest = collate_into_cbow(list(test_df['text']))\n",
    "#clf.score(xtest, codestest)\n",
    "getfscore(clf, codestest, xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5f1dcd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#intrain = [i % 5 != 0 for i in range(translated_df.size)]\n",
    "intrain = [i % 9 > 2 for i in range(translated_df.size)]\n",
    "#intrain = [True, True, True, True, True, True, False, False, False,\n",
    "#          True, True, True, False, False, False, True, True, True, \n",
    "#          False, False, False, True, True, True, True, True, True,]\n",
    "#notintrain = [i % 9 <= 2 for i in range(translated_df.size)]\n",
    "notintrain = [not element for element in intrain]\n",
    "train_df = translated_df[intrain]\n",
    "test_df = translated_df[notintrain]\n",
    "codestrain = authors[intrain]\n",
    "#codestrain = translators[intrain]\n",
    "codestest = authors[notintrain]\n",
    "#codestest = translators[notintrain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8ae5ccf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"], min_freq=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6ee70c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xtrain = collate_into_cbow(list(train_df['text']))\n",
    "xtrain = collate_into_bow(list(train_df['text']))\n",
    "clf = LogisticRegression(random_state=0, max_iter = 10000).fit(xtrain, codestrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5c50efe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14285714285714285"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#xtest = collate_into_cbow(list(test_df['text']))\n",
    "xtest = collate_into_bow(list(test_df['text']))\n",
    "clf.score(xtest, codestest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5daa9498",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24441824",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a4321a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9718, 1.0331, 0.9713,  ..., 0.9487, 0.9690, 1.0167],\n",
       "        [0.9623, 1.0266, 0.9746,  ..., 0.9461, 0.9750, 1.0369],\n",
       "        [0.9642, 1.0305, 0.9860,  ..., 0.9489, 0.9650, 1.0253],\n",
       "        ...,\n",
       "        [0.9702, 1.0277, 0.9936,  ..., 0.9510, 0.9662, 1.0180],\n",
       "        [0.9674, 1.0326, 0.9837,  ..., 0.9397, 0.9687, 1.0076],\n",
       "        [0.9584, 1.0264, 0.9794,  ..., 0.9409, 0.9689, 1.0117]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collate_into_cnbow(list(train_df['text']))#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bd68b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f84156cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1]\n",
      "[1 1 1 1 1 3 3 3 1 1 1]\n",
      "0.7272727272727273\n"
     ]
    }
   ],
   "source": [
    "#for id translators\n",
    "#codestrain = translators[intrain]\n",
    "#codestest = translators[notintrain]\n",
    "#clf = GaussianNB()\n",
    "clf = ComplementNB(force_alpha=True)\n",
    "xtrain = collate_into_bow(list(train_df['text']))\n",
    "xtest = collate_into_bow(list(test_df['text']))\n",
    "clf.fit(xtrain, codestrain)\n",
    "clf.score(xtest, codestest)\n",
    "getfscore(clf, codestest, xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4618d781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[1 1 1 1 1 1 1 2 2 2 2 2 2 2 3 3 3 3 3 3 3]\n",
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "#for id translators\n",
    "codestrain = translators[intrain]\n",
    "codestest = translators[notintrain]\n",
    "#clf = GaussianNB()\n",
    "clf = ComplementNB(force_alpha=True)\n",
    "xtrain = collate_into_bow(list(train_df['text']))\n",
    "xtest = collate_into_bow(list(test_df['text']))\n",
    "clf.fit(xtrain, codestrain)\n",
    "clf.score(xtest, codestest)\n",
    "getfscore(clf, codestest, xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a7c18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "codestrain = translators[intrain]\n",
    "codestest = translators[notintrain]\n",
    "#clf = GaussianNB()\n",
    "clf = ComplementNB(force_alpha=True)\n",
    "xtrain = collate_into_bow(list(train_df['text']))\n",
    "xtest = collate_into_bow(list(test_df['text']))\n",
    "clf.fit(xtrain, codestrain)\n",
    "clf.score(xtest, codestest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2497d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2b7e2a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 2 2 3 3 2 3 3 2 2 5 6 2 3 3 3 2 5 5 2 3]\n",
      "[1 1 1 2 2 2 2 2 2 5 5 5 3 3 3 5 5 5 5 5 5]\n",
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "#for id original author\n",
    "codestrain = authors[intrain]\n",
    "codestest = authors[notintrain]\n",
    "xtrain = collate_into_cnbow(list(train_df['text']))\n",
    "xtest = collate_into_cnbow(list(test_df['text']))\n",
    "clf = GaussianNB()\n",
    "#clf = ComplementNB(force_alpha=True)\n",
    "clf.fit(xtrain, codestrain)\n",
    "clf.score(xtest, codestest)\n",
    "getfscore(clf, codestest, xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b218669b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codestrain = authors[intrain]\n",
    "codestest = authors[notintrain]\n",
    "xtrain = collate_into_cnbow(list(train_df['text']))\n",
    "xtest = collate_into_cnbow(list(test_df['text']))\n",
    "clf = GaussianNB()\n",
    "clf.fit(xtrain, codestrain)\n",
    "clf.score(xtest, codestest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73420bac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
